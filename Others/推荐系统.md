# 基本概念
曝光 点击 浏览 点赞/收藏/转发 ……

### 链路
+ 召回
+ 粗排
+ 精排
+ 重拍

### A/B测试
将用户随机分桶，不同桶使用不同的测试。

分层。不同层之间正交，同层之间互斥。

holdout桶不做任何实验。

# 召回
### ItemCF
维护：
+ 用户到物品，最近交互过的n个物品
+ 物品到物品，相似度最高k个

线上做召回：
+ 利用两个索引，每次取回nk个物品

### Swing
计算物品相似度，考虑用户重合度。重合度高的用户权重低。

### UserCF
维护：
+ 用户到物品，最近交互过的n个物品
+ 用户到用户，相似度最高k个

线上做召回：
+ 利用两个索引，每次取回nk个物品
+ 预估user对item的兴趣分数

### 近似最近邻查找
+ 将向量划分为多个区域，每个区域对应一个向量
+ 用户找到最相似的向量，然后只和对应区域的物品计算兴趣度

### 双塔模型：模型结构、训练方法
两个神经网络单独分别计算user向量、物品向量，然后基于两个向量计算感兴趣程度。

训练：
+ 单独看待每一个样本
+ 一正一负
+ 一正多负

前期融合特征，适用于排序，计算简单。但是如果用来召回，需要把模型跑很多次，不合适。

### 双塔模型：正负样本
+ 正样本：曝光后被点击的
  + 二八原则
  + 过采样、降采样
+ 负样本
  + 简单负样本：对冷门物品不公平
    + batch内负样本，对热门物品不公平
  + 困难负样本：排序后淘汰物品
  + 混合负样本

### 双塔模型：线上服务、模型更新
+ 离线存储物品特征向量，划分区域
+ 在线计算用户向量作为query：用户特征在变化

模型更新：
+ 全量更新：用一整天的数据更新，仅1 epoch（fine tune）
  + 基于上一次全量更新，而不是增量
  + 应当使用shuffle后的整天数据
+ 增量更新：以小时为单位更新，只更新embedding
  + 只用增量，会导致数据按时间不均匀

### 双塔模型 + 自监督学习
只用来训练物品塔。

同一个物体进行特征变换，产生两个物品，他们通过神经网络之后应当有较高相似度；而不同物品之间尽量远

特征变换方法：
+ Random Mask：丢弃个别属性
+ Dropout：一个属性中丢弃个别值
+ 互补特征：分为不同集合，每次只使用一个集合中的特征
+ Mask 关联特征

### Deep Retrieval 召回
物品表示为路径。

+ 索引
  + 一个物品对应多个路径
  + 一个路径对应多个物品
+ 预估模型
  + 第一层输出一个节点
  + 前一层输出embedding之后连接到用户特征，进入下一层
+ 线上召回
  + beam search 召回一批路径
    + 设置 beam size：每两层之间找几条路径（贪心）
  + 通过路径找到一批物品
  + 排序找到子集
+ 训练
  + 神经网络参数：只用正样本
  + 物品表征学习
    + 用分数衡量item与path相关性
    + 根据分数选出 J 条路经作为 item 的表征
    + 每次固定 J-1 条，从其他随机选一个计算损失，更新集合
    + 正则化：防止一条路径里对应物品太多


### 其他各种召回
+ 地理位置召回
+ 关注/有互动作者召回
+ ……

### 曝光过滤 & Bloom Filter
已曝光的物品不应二次曝光，于是需要记录已曝光物品。

使用m维01向量，k个哈希函数，将物品ID映射到向量中k个位置。

不支持单独删除物品，只能每个一段时间重新计算。

保证不召回已曝光物品，但是会误伤未曝光物品。

# 排序
### 多目标模型
将各种特征传入NN提取高维特征，然后用不同的分类头sigmoid，得到多个[0,1]值作为点击率等的预测值。

类别不均衡：负样本多。
+ 解决：降采样，然后要根据采样率进行校准。

### Multi-gate Mixture-of-Experts (MMoE)
各种特征传入多个NN（Experts），另外根据预测任务个数，传入多个含有softmax的神经网络作为权重。用不同权重加权平均得到不同任务的预测值。

Softmax极化问题：输入接近1，使得有些专家被忽略。
+ 解决：dropout softmax的输出

### 视频播放建模
视频播放时长、完播率。

播放时长：
+ 时长直接回归拟合并不好。
+ 使用全连接层输出一个实数z，
  + $p=\frac{exp(z)}{1+exp(z)}$
  + $y=\frac{t}{1+t}$
  + 使用p和y的交叉熵作为损失函数进行梯度下降
  + 用$exp(z)$作为播放时长的预估

完播率：
+ 直接用播放时长在总时长的占比
+ 二元分类，设置阈值划分正负样本
+ 视频越长，完播率越低，因此不能直接用来计算分数，要额外正则化

### 排序模型的特征
用户画像：
+ ID
+ 统计学属性：性别、年龄
+ 账号信息：注册时间、活跃度
+ 感兴趣的话题

物品画像：
+ ID
+ 时间、地点
+ 标题、关键词

用户统计特征：
+ 最近的点击数等，实时变化的感兴趣方向

### 粗排
要求单次推理代价小，准确性可以牺牲。

双塔模型：
+ 前期融合，用于精排
+ 线上只需要用户塔进行推理

三塔模型：
+ 用户塔：可以很大，反正只推理一次
+ 物品塔：线上不用推理，只需缓存，可以比较大
+ 交叉塔：较小，输入实时变化的统计特征、交叉特征，做n次推理
+ 三者输出作为上层的推理输入

# 特征交叉
### Factorized Machine (FM) 因式分解机
基本的线性模型没有进行特征交叉。因此需要使用二阶交叉特征。

$p=b+\sum^d_{i=1}w_ix_i+\sum^d_{i=1}\sum^d_{j=i+1}u_{ij}x_ix_j$

其中二阶交叉特征的权重U是一个 d\*d 的矩阵，参数量太大，使用低秩近似：$U\approx VV^T$，其中V是 d\*k 的矩阵，k远小于d。

这样，上述式子可以转化为：

$p=b+\sum^d_{i=1}w_ix_i+\sum^d_{i=1}\sum^d_{j=i+1}v_i^Tv_jx_ix_j$

这里的$v_i$是矩阵V的第i行。这就是FM。

+ 交叉特征提升表达能力
+ 低秩近似使得参数量小，不易过拟合

### DCN 深度交叉网络
交叉层：$x_{i+1}=x_0\otimes(Wx_i+b)+x_i$。看起来是线性变换之后与输入特征进行逐元素乘法，然后残差连接。

交叉网络就是多个交叉层的连接。

深度交叉网络DCN是有一个并联的交叉网络和全连接网络，输出进行concat之后再交给第三个全连接层，得到最终的结果。

### LHUC (PPNet)
物品特征和用户特征分别进入神经网络，得到相同size输出，进行逐元素相乘得到一个输出。这个模块可以多次叠加。其中用户特征每次都用原始的，物品特征每次都用上一模块的输出。

### SENet和Bilinear交叉
输入m个离散特征，使用pooling变成一个m维向量，然后通过一些全连接层，输出还是m维，sigmoid之后成为权重。这个权重再乘以最开始的离散特征，得到和输入大小一样的m个向量

本质：对离散特征做field-wise加权。

然后需要field间特征交叉。Bilinear Cross：$f_{ij}=x^T_iW_{ij}x_j$，(1,m)\*(m,m)\*(m,1)，使得两个形状为(1,m)的向量得到交叉。

也可以使用哈达玛乘积。如果参数过多需要人工指定一些需要交叉的特征。

# 行为序列
lastN：用户最近交互的n个物品。

这n个物品通过embedding产生key，然后取平均作为用户特征。


### DIN模型：Deep Interest Network
使用attention机制。

用候选物品的向量与LastN物品向量计算相似度，然后用相似度计算加权和。

DIN需要LastN特征，而用户塔看不到LastN，所以不能用于用户塔。

### SIM模型
DIN的问题，只关注短期兴趣，长期的直接丢弃。

SIM改进DIN：
+ 保留更长的历史序列
+ 依据相似度查找TopK
  + 根据类目，基于规则
  + embedding之后把候选物品作为query，进行查询
+ 使用TopK进行计算，降低注意力的计算量

trick：可以使用时间信息，将时间信息embedding之后concat入LastN物品特征中。

# 重排（多样性）
粗排、精排的后处理。

### 物品相似性的度量
+ 基于属性标签
  + 标签由CV、NLP算法得到
+ 基于向量表征 cos相似度
  + 用召回的双塔模型学到的物品向量不好：长尾问题
  + 基于内容的向量表征好：使用CLIP预训练（xhs）

### MMR: Maximal Marginal Relevance
$MR_i=\theta\cdot reward_i-(1-\theta)\cdot max_{j\in S}sim(i,j)$

这里面S是已选中集合。相当于未选中物品与已选中物品相似度越大，越不应该被选上。

每一轮选出MR分数最高的，移入已选中集合。

当S很大的时候，很难找到不相似的物品，因此使用一个固定大小的滑动窗口，限制只和最近选中的几个物品进行计算。

### DPP：数学基础
超平形体中的点都可以用边向量加权求和计算得到，权重在[0,1]。

一组k个d维向量定义一个超平形体（k<=d）。向量之间线性独立。

定义k个物品多样性：超平形体体积。数学上可以证明行列式和体积是等价的。行列式越大越好。

### DPP：多样性算法
argmax(\theta*集合中精排得分之和+(1-\theta)log行列式)

NP-hard 很难！

贪心算法：给已取得的集合增加一项计算上面这个式子，希望它最大化。

用Cholesky分解，使行列式$A=LL^T$，其中给A添加一行一列造成的影响可以快速计算。

# 冷启动
+ UGC：用户自己上传新作品
+ PGC：平台自己上传新作品

UGC的作品冷启动是一个比较困难的问题。

### 冷启优化目标&评价指标
1. 精准推荐
2. 激励发布，流量向低曝光新笔记倾斜
3. 挖掘高潜：通过初期小流量的试探，找到高质量笔记，给与流量倾斜

评价指标：
+ 作者侧：发布渗透率=发布人数/日活、人均发布量=新发布数/日活
+ 用户侧：
  + 新笔记点击率交互率
  + 日活、月活
+ 内容侧指标：高热笔记占比

要推新物品的同时减少对老物品的影响。trade-off

### 简单的召回通道
ItenCF、UserCF需要物品用户的交互信息，是不行的。

用其他的召回方式也会缺少这方面信息而效果不好。

新笔记的ID Embedding 参数刚刚初始化还未进行更新。
+ 可以有一个default embedding
+ 利用相似高曝光topk笔记的embedding取平均

多个向量召回池。
+ 1h
+ 1d
+ 7d
+ 1m
+ ……

基于类目的召回：系统维护类目到笔记的索引，按照时间从后往前排序，从用户画像到类目，取出其前k篇召回。

### 聚类召回
事先训练神经网络把物品映射为向量。对向量做聚类，记录每个cluster的中心。

新笔记计算最接近的cluster并添加。

### Look-Alike召回
根据标签判断目标种子用户，然后使用算法找到相似的用户（他们可能标签不够完整以至于不能用规则筛选出）。

对新笔记进行交互的用户作为种子用户，进行用户扩散。可以使用用户的embedding向量计算相似度。

可以用动量加权平均的方法更新这个新笔记的种子向量，这样就不用维护一个种子用户的集合。

### 流量调控
+ 新笔记提权
  + 易实现，难调参
+ 新笔记保量：保证在一段时间里获得一定的流量，没达到预期就提高权重
  + 可以动态调整
  + 差异化保量，质量高的提升保量最低阈值

提权太狠了，推荐给非目标受众，造成点赞率等底下。

### 冷启的AB测试
AB测试考察：
+ 作者侧指标
  + 新笔记划分为实验组和对照组，他们之间相互竞争并不影响总体上划分给新笔记的流量占比
  + 新笔记之间抢流量
  + 新笔记跟老笔记抢流量
  + 如果过分划分，召回笔记池太小，用户体验下降
+ 用户侧指标

冷启影响用户体验，实验组数据变差，但是由于保量目标大多由实验组承担，对照组体验提升。

# 涨指标的方法
### 概述
最重要的指标：日活DAU和留存（电商而言应当是交易额之类的）

LT7/LT30：用户在今天之后7/30天内登陆的次数均值。显然这个指标需要7/30天之后才能知道。如果LT升而DAU下降了，只能说低活用户走了。

### 召回
推荐系统有几十条召回通道，召回总量是固定的。总量越大，指标越好；但是粗排计算量变大。

双塔模型和I2I模型。

+ 有很多小众的模型，占比小；
+ 有很多内容池；
+ 一个模型应用于多个内容池，得到多条召回通道；只训练一次。

改进方向：
+ 优化正样本、负样本
+ 改进神经网络结构
  + 多向量模型：用户塔输出很多和物品塔输出大小一样的向量，每个向量单独作为一个指标的预估
  + 物品塔如果多输出，则保存特征的开销太大
+ 改进训练方法  
  + 二分类、batch内负采样
  + 自监督学习，提升冷门物体embedding质量

Item-2-Item：
+ U2I2I
+ U2A2I
+ U2A2A2I

### 排序
精排改进：
+ 依据算力增大网络
+ 特征交叉
+ 特征工程
+ 多目标预估

粗排：
+ 精排的改进方法
+ 三塔模型
+ 蒸馏精排模型来训练粗排模型

用户行为序列建模：
+ 增加序列长度
+ 快速筛选之后交给DIN
+ 使用除物品ID之外的特征
  
在线学习（增量更新）：
+ 有n个算力，线上只能测试n-2个新模型，另有推全的模型和holdout占用两个算力

老汤模型：
+ 老模型吃的数据多，可能比更优的新模型还要好
+ 解决：FC都随机初始化；Embedding都用训练好的
+ 老模型蒸馏新模型

### 多样性
精排使用滑动窗口，为了曝光，相邻的不能一样；粗排可以不用，因为只是给精排提供数据。

粗排同时提供分数最高的几百个；以及分数+多样性最高的几百个。

召回的多样性：根据用户向量表征体现出的兴趣范围大小，增加一定的噪声：
+ 用户兴趣窄，往用户向量加更多噪声，提升多样性
+ 用随机抽样进行用户行为序列建模
+ 探索流量：维护一个精选内容池，每次从中随机抽样几个，跳过排序直接插入最终排序结果


### 特殊用户人群
特殊用户历史行为少，个性化不准；只能提升质量来补偿。

+ 构造特殊内容池：弱个性化，只针对一个人群
+ 特殊的排序策略：
  + 主要目的是留存，少广告，少低质，少探索多样性
  + 差异化融分公式，提升点击率的权重：至少你得看才会有下一步
+ 差异化的排序模型：
  + 大模型：用全体用户行为训练拟合用户真是行为
  + 小模型：用特殊用户行为训练，拟合大模型对特殊人群的行为预估残差
  + 类似MMoE，用多个Experts

### 交互行为
+ 关注：对低粉作者激励很大，通过作者粉丝数加权
+ 转发：通过转发行为历史带来的流量进行打分并加权（KOL）
+ 评论：根据现有评论数多少、用户评论积极性、评论质量加权

# 具体模型
### Collaborative Filtering 协同过滤 CF
+ UserCF
  + 使用U-U矩阵
    + 输入用户行为日志，输出用户相似度矩阵
    + 均值：一个用户对所有视频的评价
    + Pearson系数：协方差与标准差的商
      + 具体而言，用两个用户交互过的物品计算协方差，用用户自己的交互物品集合计算各自的标准差
      + 计算协方差使用的均值是用户自己的全部数据均值
    + 余弦相似度：向量点积除以L2范数
  + 基于最相似的k个用户在某个item上，用用户相似度加权平均来预测用户对该项目的分值
  + 用户的新行为不一定导致推荐结果立即变化
  + 问题：对数据稀疏敏感、可扩展性不强、新用户冷启动难
+ ItemCF
  + 使用V-V矩阵
    + 输入用户行为日志，输出视频相似度矩阵
    + 均值：一个视频收获的所有评价
  + 用物品相似度代替用户相似度
    + 物品相似度是不变的，因此可以离线计算；用户相似度一直在变化
  + 用户的新行为一定导致推荐立即变化
  + 问题：视频冷启动；在商城里是不是好些？

### Matrix Factorization 矩阵分解 MF
在稀疏矩阵用户-物品表格中，预测空位置的表项应该是什么。

将每一个用户和物品都用一个k维向量表示，则可以划分出u\*k的用户矩阵P和i\*k的物品矩阵Q。则可以认为预测矩阵为$PQ^T$。用梯度下降更新。

优化方法：
+ 防止过拟合，损失函数加入正则项
+ 预测分数的公式不只是两个k维向量点乘，加入一些表征平台均值、用户均值、物品均值的内容

优势：
+ 缓解数据稀疏问题
+ 隐变量表示能力可以比较好
+ 空间复杂度低
+ 可扩展性高

问题：
+ 可解释性差
+ 依然没有用到物品和用户本身的显式特征
+ 随时间变化慢

### Factorization Machine 因式分解机 FM
基本的线性模型没有进行特征交叉。因此需要使用二阶交叉特征。

$p=b+\sum^d_{i=1}w_ix_i+\sum^d_{i=1}\sum^d_{j=i+1}u_{ij}x_ix_j$

其中二阶交叉特征的权重U是一个 d\*d 的矩阵，参数量太大，使用低秩近似：$U\approx VV^T$，其中V是 d\*k 的矩阵，k远小于d。

这样，上述式子可以转化为：

$p=b+\sum^d_{i=1}w_ix_i+\sum^d_{i=1}\sum^d_{j=i+1}v_i^Tv_jx_ix_j$

这里的$v_i$是矩阵V的第i行。这就是FM。

+ 交叉特征提升表达能力
+ 低秩近似使得参数量小，不易过拟合

这里有点像隐向量和embedding，给每一个特征设置一个embedding，只需要计算特征嵌入向量之间的相似度（点积）即可。

优势：
+ 缓解数据稀疏问题
+ 特征交叉

### Deep & Wide
+ Wide部分的作用是让模型具有较强的“记忆能力”(memorization)
  + 也就是基于历史数据进行相似性推荐
  + 类似CF
+ Deep部分的作用是让模型具有“泛化能力”(generalization)
  + 根据已有信息学习隐变量、高维特征表示
  + 类似MF、FM

### word2vec
[知乎](https://zhuanlan.zhihu.com/p/114538417)

分为CBOW和Skip-gram两步：
+ CBOW；已知上下文推断当前词
+ Skip-gram：已知当前词推断上下文

优化方法：
+ Huffman树近似softmax，把计算量从O(V)变成O(logV)。
+ 子采样：防止类似“the”这样的词语占据太多，出现频率越大，越有可能忽略这些词的上下文
+ 负采样：每个样本只更新一部分参数：ground-truth是one-hot，从中选择为1的正样本和几个为0的负样本，只用这些更新梯度。




